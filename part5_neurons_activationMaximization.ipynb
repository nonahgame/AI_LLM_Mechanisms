{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nonahgame/AI_LLM_Mechanisms/blob/main/part5_neurons_activationMaximization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZA5rrVINbO1"
      },
      "source": [
        "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
        "|-|:-:|\n",
        "|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n",
        "|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n",
        "|<h2>Lecture:</h2>|<h1><b>Activation maximization (code implementation)<b></h1>|\n",
        "\n",
        "<br>\n",
        "\n",
        "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
        "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
        "<i>Using the code without the course may lead to confusion or errors.</i>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BvQj17hzqwM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "\n",
        "# vector plots\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEwYHoqWz0nB"
      },
      "outputs": [],
      "source": [
        "# load GPT2 model and tokenizer\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# use GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# move the model to the GPU\n",
        "model = model.to(device)\n",
        "model.eval() # we're training the embeddings, not the model!\n",
        "\n",
        "\n",
        "# a copy of the original embeddings\n",
        "embeddings = model.wte.weight.detach().cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlHBlSYw2kyE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIbzBVCF2kvm"
      },
      "source": [
        "# Initialize a random embeddings pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLDp3Ffw2m1X"
      },
      "outputs": [],
      "source": [
        "# length of the token sequence\n",
        "seq_len = 5\n",
        "\n",
        "# random embeddings with gradient tracking\n",
        "optimized_embeddings = torch.randn((1, seq_len, embeddings.shape[1]), requires_grad=True, device=device)\n",
        "\n",
        "# normalize the std to that of the real embeddings matrix\n",
        "torch.nn.init.normal_(optimized_embeddings, mean=0, std=torch.std(embeddings))\n",
        "\n",
        "# check the shape\n",
        "optimized_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1nsbiwF3dXv"
      },
      "outputs": [],
      "source": [
        "# get the histogram values\n",
        "ye,xe = np.histogram(embeddings.flatten(),bins=80)\n",
        "yo,xo = np.histogram(optimized_embeddings.flatten().detach().cpu(),bins=80)\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(xe[:-1],ye/np.max(ye),linewidth=2,label='Embeddings matrix')\n",
        "plt.plot(xo[:-1],yo/np.max(yo),linewidth=2,label='Random matrix')\n",
        "\n",
        "plt.legend()\n",
        "plt.gca().set(xlabel='Weight values',ylabel='Frequency (max-norm)',xlim=xe[[0,-1]])\n",
        "# plt.yscale('log') # optional, gives a better appreciation of the tails\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biob5FC9twKU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMHzIzbXtwFA"
      },
      "source": [
        "# How to input embeddings in the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPUDU4uety-G"
      },
      "outputs": [],
      "source": [
        "# select a dimension to maximize\n",
        "layer_idx = 8 # 8th transformer block with index 7\n",
        "dim_idx = 91"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DY0-K4c9bnT"
      },
      "outputs": [],
      "source": [
        "# how to use the maximized embeddings\n",
        "outputs = model(\n",
        "    inputs_embeds = optimized_embeddings, # instead of input_ids\n",
        "    output_hidden_states = True # request all activations exported\n",
        "    )\n",
        "\n",
        "# the output\n",
        "print(f'Size of outputs.hidden_states: {len(outputs.hidden_states)}')\n",
        "print(f'e.g., size of activation from layer {layer_idx}: {outputs.hidden_states[layer_idx].shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an_64B6IzyQ5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_dphViu6L-j"
      },
      "source": [
        "# Now for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG2L6_0rzsWI"
      },
      "outputs": [],
      "source": [
        "n_steps = 500   # optimization steps\n",
        "lr = .001       # learning rate\n",
        "lambda_l2 = .01 # regularization amount\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam([optimized_embeddings], lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocWJVLUZz6P3"
      },
      "outputs": [],
      "source": [
        "# initialize vectors to store progress\n",
        "activationVal = np.zeros(n_steps)\n",
        "gradientNorm = np.zeros(n_steps)\n",
        "\n",
        "\n",
        "# loop over training steps\n",
        "for step in range(n_steps):\n",
        "\n",
        "  # clear gradient\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # patch embeddings directly into the model\n",
        "  outputs = model(\n",
        "      inputs_embeds = optimized_embeddings,\n",
        "      output_hidden_states = True)\n",
        "\n",
        "  # extract the dimension's activation (averaged over tokens)\n",
        "  allActivations = outputs.hidden_states[layer_idx]\n",
        "  dim_activation = allActivations[0,:,dim_idx].mean()\n",
        "\n",
        "  # squared Euclidean distance for L2 normalization\n",
        "  L2 = lambda_l2 * torch.sum(optimized_embeddings**2)\n",
        "\n",
        "  # minimize loss -> maximize activation\n",
        "  loss = -dim_activation + L2\n",
        "  activationVal[step] = dim_activation.item()\n",
        "\n",
        "\n",
        "  # run gradient descent\n",
        "  loss.backward()\n",
        "\n",
        "  # get the gradient norm\n",
        "  gradientNorm[step] = optimized_embeddings.grad.norm().item()\n",
        "\n",
        "  # finish backprop\n",
        "  optimizer.step()\n",
        "\n",
        "  if step%23==0:\n",
        "    print(f'Step {step:4}/{n_steps}, Target activation: {activationVal[step]:6.2f} (vs. neighbor: {allActivations[0,:,dim_idx+1].mean():.2f})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB5DgB8f7VwE"
      },
      "outputs": [],
      "source": [
        "_,axs = plt.subplots(1,2,figsize=(12,3))\n",
        "\n",
        "# plot the activation magnitudes\n",
        "axs[0].plot(activationVal,'o',markersize=4,markerfacecolor=[.7,.6,.9],markeredgecolor='none')\n",
        "axs[0].set(xlabel='Training steps',ylabel='Dimension activation',title='\"Inverse loss\" optimization')\n",
        "\n",
        "# plot the gradient norms\n",
        "axs[1].plot(gradientNorm,'o',markersize=4,markerfacecolor=[.9,.6,.7],markeredgecolor='none')\n",
        "axs[1].set(xlabel='Training steps',ylabel='Embedding gradient norm',title='Norm of gradients')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iekRfCOi7VtP"
      },
      "outputs": [],
      "source": [
        "# redraw the histograms of embedding values\n",
        "\n",
        "# get the histogram values\n",
        "yo2,xo2 = np.histogram(optimized_embeddings.flatten().detach().cpu(),bins=80)\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(xe[:-1],ye/np.max(ye),linewidth=2,label='Embeddings matrix')\n",
        "plt.plot(xo[:-1],yo/np.max(yo),linewidth=2,label='Random matrix')\n",
        "plt.plot(xo2[:-1],yo2/np.max(yo2),linewidth=2,label='Optimized matrix')\n",
        "\n",
        "plt.legend()\n",
        "plt.gca().set(xlabel='Weight values',ylabel='Frequency (max-norm)',\n",
        "              xlim=[-1,1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ariOrGfk8Lxj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "plt.imshow(optimized_embeddings.squeeze().detach().cpu(),\n",
        "           aspect='auto',vmin=-.3,vmax=.3,origin='lower')\n",
        "\n",
        "plt.gca().set(xlabel='Embedding dim.',ylabel='Token position')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCn2piSK8Lu0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYtlUFOu9XvD"
      },
      "source": [
        "# Find closest tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa-WrtxcA4AM"
      },
      "outputs": [],
      "source": [
        "# one embed\n",
        "oneemb = optimized_embeddings[0][0].detach().cpu()\n",
        "\n",
        "# cosine similarity with all embedding vectors\n",
        "cs = F.cosine_similarity(oneemb.unsqueeze(0), embeddings)\n",
        "\n",
        "# find the token with max cossim\n",
        "maxtok = np.argmax(cs)\n",
        "\n",
        "# and visualize\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(cs,'ko',markerfacecolor=[.9,.7,.8,.6])\n",
        "plt.gca().set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Cosine similarity',\n",
        "              title=f'Similarities to all token embeddings (top token is \"{tokenizer.decode(maxtok)}\")')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hV3-xvPz77c"
      },
      "outputs": [],
      "source": [
        "# decode embeddings to closest tokens\n",
        "optimized_tokens = []\n",
        "\n",
        "for emb in optimized_embeddings[0]:\n",
        "\n",
        "  # cosine similarity with embedding weights\n",
        "  similarities = F.cosine_similarity(emb.unsqueeze(0).detach().cpu(), embeddings)\n",
        "\n",
        "  # find the max similarity\n",
        "  maxtok = np.argmax(similarities)\n",
        "  optimized_tokens.append(maxtok)\n",
        "\n",
        "print('Optimized token sequence:\\n',tokenizer.decode(optimized_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAz84yHT0foh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}